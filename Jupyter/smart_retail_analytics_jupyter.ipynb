{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env FACE_MODEL=/opt/intel/openvino/deployment_tools/open_model_zoo/tools/downloader/intel/face-detection-adas-0001/FP32/face-detection-adas-0001.xml\n",
    "%env POSE_MODEL=/opt/intel/openvino/deployment_tools/open_model_zoo/tools/downloader/intel/head-pose-estimation-adas-0001/FP32/head-pose-estimation-adas-0001.xml\n",
    "%env MOOD_MODEL=/opt/intel/openvino/deployment_tools/open_model_zoo/tools/downloader/intel/emotions-recognition-retail-0003/FP32/emotions-recognition-retail-0003.xml\n",
    "%env PERSON_MODEL=/opt/intel/openvino/deployment_tools/open_model_zoo/tools/downloader/intel/person-detection-retail-0002/FP32/person-detection-retail-0002.xml\n",
    "%env OBJ_MODEL=../resources/FP32/mobilenet-ssd.xml\n",
    "%env CPU_EXTENSION=/opt/intel/openvino/inference_engine/lib/intel64/libcpu_extension_sse4.so\n",
    "%env LABEL_FILE=../resources/labels.txt\n",
    "%env FLAG=async\n",
    "%env FACE_DEVICE=CPU\n",
    "%env MOOD_DEVICE=CPU\n",
    "%env POSE_DEVICE=CPU\n",
    "%env PERSON_DEVICE=CPU\n",
    "%env OBJ_DEVICE=CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    " Copyright (c) 2018 Intel Corporation.\n",
    " Permission is hereby granted, free of charge, to any person obtaining\n",
    " a copy of this software and associated documentation files (the\n",
    " \"Software\"), to deal in the Software without restriction, including\n",
    " without limitation the rights to use, copy, modify, merge, publish,\n",
    " distribute, sublicense, and/or sell copies of the Software, and to\n",
    " permit persons to whom the Software is furnished to do so, subject to\n",
    " the following conditions:\n",
    " The above copyright notice and this permission notice shall be\n",
    " included in all copies or substantial portions of the Software.\n",
    " THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
    " EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    " MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n",
    " NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n",
    " LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n",
    " OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n",
    " WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from argparse import ArgumentParser\n",
    "import logging as log\n",
    "import numpy as np\n",
    "from inference import Network\n",
    "from influxdb import InfluxDBClient\n",
    "from flask import Flask, render_template, Response\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "# Constants\n",
    "CONFIG_FILE = \"../resources/config.json\"\n",
    "MAX_FRAME_GONE = 3\n",
    "INTEREST_COUNT_TIME = 5\n",
    "SENTIMENT_LABEL = ['neutral', 'happy', 'sad', 'surprise', 'anger']\n",
    "IPADDRESS = \"localhost\"\n",
    "PORT = 8086\n",
    "DATABASE_NAME = \"Retail_Analytics\"\n",
    "CENTROID_DISTANCE = 150\n",
    "\n",
    "# Global variables\n",
    "check_feed_type = [False, False, False]  # [shopper, traffic, shelf]\n",
    "centroids = []\n",
    "tracked_person = []\n",
    "person_id = 0\n",
    "interested = 0\n",
    "not_interested = 0\n",
    "db_client = None\n",
    "myriad_plugin = None\n",
    "Point = namedtuple(\"Point\", \"x,y\")\n",
    "accepted_devices = ['CPU', 'GPU', 'MYRIAD', 'HETERO:FPGA,CPU', 'HDDL']\n",
    "is_async_mode = True\n",
    "template_dir = os.path.abspath('../templates')\n",
    "\n",
    "class Centroid:\n",
    "    \"\"\"\n",
    "    Store centroid details of the face detected for tracking\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p_id, point, gone_count):\n",
    "        self.id = p_id\n",
    "        self.point = point\n",
    "        self.gone_count = gone_count\n",
    "\n",
    "\n",
    "class Person:\n",
    "    \"\"\"\n",
    "    Store the data of the people for tracking\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p_id, in_time):\n",
    "        self.id = p_id\n",
    "        self.counted = False\n",
    "        self.gone = False\n",
    "        self.in_time = in_time\n",
    "        self.out_time = None\n",
    "        self.looking = 0\n",
    "        self.positive = 0\n",
    "        self.negative = 0\n",
    "        self.neutral = 0\n",
    "        self.sentiment = ''\n",
    "\n",
    "\n",
    "class VideoCap:\n",
    "    \"\"\"\n",
    "    Store the data and manage multiple input video feeds\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_name, input_number, feed_type, labels=[]):\n",
    "        self.vc = cv2.VideoCapture(input_name)\n",
    "        self.input_number = input_number\n",
    "        self.type = feed_type\n",
    "        self.infer_network = None\n",
    "        self.nchw = []\n",
    "        self.utime = time.time()\n",
    "        self.curr_req = 0\n",
    "        self.next_req = 1\n",
    "\n",
    "        if self.type == 'shopper':\n",
    "            self.nchw_hp = []\n",
    "            self.nchw_md = []\n",
    "            self.thresh = 0.7\n",
    "\n",
    "        if self.type == 'shelf' or self.type == 'traffic':\n",
    "            self.thresh = 0.145\n",
    "            self.labels = labels\n",
    "            self.labels_map = []\n",
    "            self.last_correct_count = [0] * len(self.labels)\n",
    "            self.total_count = [0] * len(self.labels)\n",
    "            self.current_count = [0] * len(self.labels)\n",
    "            self.changed_count = [False] * len(self.labels)\n",
    "            self.candidate_count = [0] * len(self.labels)\n",
    "            self.candidate_confidence = [0] * len(self.labels)\n",
    "            self.CONF_CANDIDATE_CONFIDENCE = 6\n",
    "\n",
    "            if self.type == 'traffic':\n",
    "                self.mog = cv2.createBackgroundSubtractorMOG2()\n",
    "                self.CONF_CANDIDATE_CONFIDENCE = 3\n",
    "                self.thresh = 0.45\n",
    "\n",
    "\n",
    "def parse_conf_file():\n",
    "    \"\"\"\n",
    "    Parse the configuration file and store the data in VideoCap object\n",
    "\n",
    "    :return video_caps: List of VideoCap object containing input stream data\n",
    "    \"\"\"\n",
    "    global CONFIG_FILE\n",
    "    global check_feed_type\n",
    "\n",
    "    video_caps = []\n",
    "\n",
    "    assert os.path.isfile(CONFIG_FILE), \"{} file doesn't exist\".format(CONFIG_FILE)\n",
    "    config = json.loads(open(CONFIG_FILE).read())\n",
    "    for idx, item in enumerate(config['inputs']):\n",
    "        labels = []\n",
    "        parse_video = item['video']\n",
    "        input_number = idx + 1\n",
    "        if 'type' in item.keys():\n",
    "            feed_type = item['type']\n",
    "            if len(feed_type) == 0:\n",
    "                print(\"Ignoring video {}... Format error\".format(parse_video))\n",
    "                continue\n",
    "            if feed_type == 'shelf':\n",
    "                check_feed_type[2] = True\n",
    "                if 'label' in item.keys():\n",
    "                    labels = [item['label']]\n",
    "                    if len(labels) == 0:\n",
    "                        print(\"Ignoring video {}... Format error\".format(parse_video))\n",
    "                        continue\n",
    "                else:\n",
    "                    print(\"Format error while reading labels for {}\".format(feed_type))\n",
    "                    continue\n",
    "            elif feed_type == 'traffic':\n",
    "                check_feed_type[1] = True\n",
    "                labels = ['person']\n",
    "            elif feed_type == 'shopper':\n",
    "                check_feed_type[0] = True\n",
    "            if parse_video.isdigit():\n",
    "                video_cap = VideoCap(int(parse_video), input_number, feed_type, labels)\n",
    "            else:\n",
    "                assert os.path.isfile(parse_video), \"{} doesn't exist\".format(parse_video)\n",
    "                video_cap = VideoCap(parse_video, input_number, feed_type, labels)\n",
    "            video_cap.input_name = parse_video\n",
    "            video_caps.append(video_cap)\n",
    "        else:\n",
    "            print(\"Feed type not specified for \", parse_video)\n",
    "\n",
    "    for video_cap in video_caps:\n",
    "        assert video_cap.vc.isOpened(), \"Could not open {} for reading\".format(video_cap.input_name)\n",
    "        video_cap.input_width = video_cap.vc.get(3)\n",
    "        video_cap.input_height = video_cap.vc.get(4)\n",
    "        if video_cap.type == 'traffic':\n",
    "            video_cap.accumulated_frame = np.zeros(\n",
    "                (int(video_cap.input_height), int(video_cap.input_width)), np.uint8)\n",
    "\n",
    "    return video_caps\n",
    "\n",
    "\n",
    "def load_model_device(infer_network, model, device, in_size, out_size, num_requests, cpu_extension, tag):\n",
    "    \"\"\"\n",
    "    Loads the networks\n",
    "\n",
    "    :param infer_network: Object of the Network() class\n",
    "    :param model: .xml file of pre trained model\n",
    "    :param device: Target device\n",
    "    :param in_size: Number of input layers\n",
    "    :param out_size: Number of output layers\n",
    "    :param num_requests: Index of Infer request value. Limited to device capabilities\n",
    "    :param cpu_extension: extension for the CPU device\n",
    "    :return:  Shape of input layer\n",
    "    \"\"\"\n",
    "    if 'MULTI' not in device and device not in accepted_devices:\n",
    "        print(\"Unsupported device: \" + device)\n",
    "        sys.exit(1)\n",
    "    elif 'MULTI' in device:\n",
    "        target_devices = device.split(':')[1].split(',')\n",
    "        for multi_device in target_devices:\n",
    "            if multi_device not in accepted_devices:\n",
    "                print(\"Unsupported device: \" + device)\n",
    "                sys.exit(1)\n",
    "\n",
    "    global myriad_plugin\n",
    "    if device == 'MYRIAD':\n",
    "        if myriad_plugin is None:\n",
    "            myriad_plugin, (nchw) = infer_network.load_model(model, device, in_size, out_size, num_requests)\n",
    "        else:\n",
    "            nchw = infer_network.load_model(model, device, in_size, out_size, num_requests, plugin=myriad_plugin)[1]\n",
    "    else:\n",
    "        nchw = infer_network.load_model(model, device, in_size, out_size, num_requests, cpu_extension, tag)[1]\n",
    "\n",
    "    return nchw\n",
    "\n",
    "\n",
    "def load_models(video_caps):\n",
    "    \"\"\"\n",
    "    Load the required models\n",
    "\n",
    "    :param video_caps: List of VideoCap objects\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    global check_feed_type\n",
    "    plugin = None\n",
    "\n",
    "    face_device = os.environ['FACE_DEVICE'] if 'FACE_DEVICE' in os.environ.keys() else \"CPU\"\n",
    "    mood_device = os.environ['MOOD_DEVICE'] if 'MOOD_DEVICE' in os.environ.keys() else \"CPU\"\n",
    "    pose_device = os.environ['POSE_DEVICE'] if 'POSE_DEVICE' in os.environ.keys() else \"CPU\"\n",
    "    obj_device = os.environ['OBJ_DEVICE'] if 'OBJ_DEVICE' in os.environ.keys() else \"CPU\"\n",
    "    person_device = os.environ['PERSON_DEVICE'] if 'PERSON_DEVICE' in os.environ.keys() else \"CPU\"\n",
    "\n",
    "    cpu_extension = os.environ['CPU_EXTENSION'] if 'CPU_EXTENSION' in os.environ.keys() else None\n",
    "    face_model = os.environ['FACE_MODEL'] if 'FACE_MODEL' in os.environ.keys() else None\n",
    "    pose_model = os.environ['POSE_MODEL'] if 'POSE_MODEL' in os.environ.keys() else None\n",
    "    mood_model = os.environ['MOOD_MODEL'] if 'MOOD_MODEL' in os.environ.keys() else None\n",
    "    obj_model = os.environ['OBJ_MODEL'] if 'OBJ_MODEL' in os.environ.keys() else None\n",
    "    person_model = os.environ['PERSON_MODEL'] if 'PERSON_MODEL' in os.environ.keys() else None\n",
    "\n",
    "    # Check if one the feed type is \"shopper\". If yes, load the face, head pose and mood detection model\n",
    "    if check_feed_type[0]:\n",
    "        assert face_model, 'Please specify the path to face detection model using the environment variable FACE_MODEL'\n",
    "        assert pose_model, 'Please specify the path to head pose model using the environment variable POSE_MODEL'\n",
    "        assert mood_model, 'Please specify the path to mood detection model using the environment variable MOOD_MODEL'\n",
    "        \n",
    "        infer_network_face = Network()\n",
    "        infer_network_pose = Network()\n",
    "        infer_network_mood = Network()\n",
    "\n",
    "        tag_face = {\"VPU_HDDL_GRAPH_TAG\":\"tagFace\"}\n",
    "        tag_pose = {\"VPU_HDDL_GRAPH_TAG\":\"tagPose\"}\n",
    "        tag_mood = {\"VPU_HDDL_GRAPH_TAG\":\"tagMood\"}\n",
    "        nchw_fd = load_model_device(infer_network_face, face_model, face_device, 1, 1, 2, cpu_extension, tag_face)\n",
    "        nchw_hp = load_model_device(infer_network_pose, pose_model, pose_device, 1, 3, 2, cpu_extension, tag_pose)\n",
    "        nchw_md = load_model_device(infer_network_mood, mood_model, mood_device, 1, 1, 2, cpu_extension, tag_mood)\n",
    "\n",
    "    # Check if one the feed type is \"traffic\" or \"shelf\". If yes, load the mobilenet-ssd model\n",
    "    if check_feed_type[2]:\n",
    "        infer_network = Network()\n",
    "        tag_obj = {\"VPU_HDDL_GRAPH_TAG\":\"tagMobile\"}\n",
    "        nchw = load_model_device(infer_network, obj_model, obj_device, 1, 1, 2, cpu_extension, tag_obj)\n",
    "    if check_feed_type[1]:\n",
    "        infer_network_person = Network()\n",
    "        tag_person = {\"VPU_HDDL_GRAPH_TAG\":\"tagPerson\"}\n",
    "        nchw_pr = load_model_device(infer_network_person, person_model, person_device, 2, 1, 2,\n",
    "                                    cpu_extension, tag_person)\n",
    "    for video_cap in video_caps:\n",
    "        if video_cap.type == 'shopper':\n",
    "            video_cap.infer_network = infer_network_face\n",
    "            video_cap.infer_network_hp = infer_network_pose\n",
    "            video_cap.infer_network_md = infer_network_mood\n",
    "            video_cap.nchw.extend(nchw_fd)\n",
    "            video_cap.nchw_hp.extend(nchw_hp)\n",
    "            video_cap.nchw_md.extend(nchw_md)\n",
    "\n",
    "        if video_cap.type == 'shelf':\n",
    "            video_cap.infer_network = infer_network\n",
    "            video_cap.nchw.extend(nchw)\n",
    "        if video_cap.type == 'traffic':\n",
    "            video_cap.infer_network = infer_network_person\n",
    "            video_cap.nchw.extend(nchw_pr)\n",
    "\n",
    "\n",
    "def object_detection(video_cap, res):\n",
    "    \"\"\"\n",
    "    Parse the inference result to get the detected object\n",
    "\n",
    "    :param video_cap: VideoCap object of the frame on which object is detected\n",
    "    :param res: Inference output\n",
    "    :return obj_det: List of coordinates of bounding boxes of the objects detected\n",
    "    \"\"\"\n",
    "    obj_det = []\n",
    "\n",
    "    for obj in res[0][0]:\n",
    "        label = int(obj[1]) - 1\n",
    "\n",
    "        # Draw objects only when probability is more than specified threshold\n",
    "        if obj[2] > video_cap.thresh:\n",
    "            # If the feed type is traffic shelf, look only for the person\n",
    "            if  video_cap.type == 'traffic' and label == 0:\n",
    "                video_cap.current_count[label] += 1\n",
    "\n",
    "            # If the feed type is traffic or shelf, look only for the objects specified by the user\n",
    "            if video_cap.type == 'shelf':\n",
    "                if label not in video_cap.labels_map:\n",
    "                    continue\n",
    "                label_idx = video_cap.labels_map.index(label)\n",
    "                video_cap.current_count[label_idx] += 1\n",
    "\n",
    "            if obj[3] < 0:\n",
    "                obj[3] = 0\n",
    "            if obj[4] < 0:\n",
    "                obj[4] = 0\n",
    "\n",
    "            xmin = int(obj[3] * video_cap.input_width)\n",
    "            ymin = int(obj[4] * video_cap.input_height)\n",
    "            xmax = int(obj[5] * video_cap.input_width)\n",
    "            ymax = int(obj[6] * video_cap.input_height)\n",
    "            obj_det.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "    return obj_det\n",
    "\n",
    "\n",
    "def get_used_labels(video_caps):\n",
    "    \"\"\"\n",
    "    Read the model's label file and get the position of labels required by the application\n",
    "\n",
    "    :param video_caps: List of VideoCap objects\n",
    "    :return labels: List of labels present in the label file\n",
    "    \"\"\"\n",
    "    global check_feed_type\n",
    "\n",
    "    if check_feed_type[1] is False and check_feed_type[2] is False:\n",
    "        return\n",
    "\n",
    "    label_file = os.environ['LABEL_FILE'] if 'LABEL_FILE' in os.environ.keys() else None\n",
    "    assert label_file, \"Please specify the path label file using the environmental variable LABEL_FILE\"\n",
    "    assert os.path.isfile(label_file), \"{} file doesn't exist\".format(label_file)\n",
    "    with open(label_file, 'r') as label_file:\n",
    "        labels = [x.strip() for x in label_file]\n",
    "\n",
    "    assert labels != [], \"No labels found in {} file\".format(label_file)\n",
    "    for video_cap in video_caps:\n",
    "        if video_cap.type == 'shelf' or video_cap.type == 'traffic':\n",
    "            for label in video_cap.labels:\n",
    "                if label in labels:\n",
    "                    label_idx = labels.index(label)\n",
    "                    video_cap.labels_map.append(label_idx)\n",
    "                else:\n",
    "                    video_cap.labels_map.append(False)\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def process_output(video_cap):\n",
    "    \"\"\"\n",
    "    Count the number of object detected\n",
    "\n",
    "    :param video_cap: VideoCap object\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for i in range(len(video_cap.labels)):\n",
    "        if video_cap.candidate_count[i] == video_cap.current_count[i]:\n",
    "            video_cap.candidate_confidence[i] += 1\n",
    "        else:\n",
    "            video_cap.candidate_confidence[i] = 0\n",
    "            video_cap.candidate_count[i] = video_cap.current_count[i]\n",
    "\n",
    "        if video_cap.candidate_confidence[i] == video_cap.CONF_CANDIDATE_CONFIDENCE:\n",
    "            video_cap.candidate_confidence[i] = 0\n",
    "            video_cap.changed_count[i] = True\n",
    "        else:\n",
    "            continue\n",
    "        if video_cap.current_count[i] > video_cap.last_correct_count[i]:\n",
    "            video_cap.total_count[i] += video_cap.current_count[i] - video_cap.last_correct_count[i]\n",
    "\n",
    "        video_cap.last_correct_count[i] = video_cap.current_count[i]\n",
    "\n",
    "\n",
    "def remove_centroid(p_id):\n",
    "    \"\"\"\n",
    "    Remove the centroid from the \"centroids\" list when the person is out of the frame and\n",
    "    set the person.gone variable as true\n",
    "\n",
    "    :param p_id: ID of the person whose centroid data has to be deleted\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    global centroids\n",
    "    global tracked_person\n",
    "\n",
    "    for idx, centroid in enumerate(centroids):\n",
    "        if centroid.id is p_id:\n",
    "            del centroids[idx]\n",
    "            break\n",
    "\n",
    "    if tracked_person[p_id]:\n",
    "        tracked_person[p_id].gone = True\n",
    "        tracked_person[p_id].out_time = time.time()\n",
    "\n",
    "\n",
    "def add_centroid(point):\n",
    "    \"\"\"\n",
    "    Add the centroid of the object to the \"centroids\" list\n",
    "\n",
    "    :param point: Centroid point to be added\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    global person_id\n",
    "    global centroids\n",
    "    global tracked_person\n",
    "\n",
    "    centroid = Centroid(person_id, point, gone_count=0)\n",
    "    person = Person(person_id, time.time())\n",
    "    centroids.append(centroid)\n",
    "    tracked_person.append(person)\n",
    "    person_id += 1\n",
    "\n",
    "\n",
    "def closest_centroid(point):\n",
    "    \"\"\"\n",
    "    Find the closest centroid\n",
    "\n",
    "    :param point: Coordinate of the point for which the closest centroid point has to be detected\n",
    "    :return p_idx: Id of the closest centroid\n",
    "            dist: Distance of point from the closest centroid\n",
    "    \"\"\"\n",
    "    global centroids\n",
    "    p_idx = 0\n",
    "    dist = sys.float_info.max\n",
    "\n",
    "    for idx, centroid in enumerate(centroids):\n",
    "        _point = centroid.point\n",
    "        dx = point.x - _point.x\n",
    "        dy = point.y - _point.y\n",
    "        _dist = math.sqrt(dx * dx + dy * dy)\n",
    "        if _dist < dist:\n",
    "            dist = _dist\n",
    "            p_idx = centroid.id\n",
    "\n",
    "    return [p_idx, dist]\n",
    "\n",
    "\n",
    "def update_centroid(points, looking, sentiment, fps):\n",
    "    \"\"\"\n",
    "    Update the centroid data in the centroids list and check whether the person is interested or not interested\n",
    "\n",
    "    :param points: List of centroids of the faces detected\n",
    "    :param looking: List of bool values indicating if the person is looking at the camera or not\n",
    "    :param sentiment: List containing the mood of the people looking at the camera\n",
    "    :param fps: FPS of the input stream\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    global MAX_FRAME_GONE\n",
    "    global INTEREST_COUNT_TIME\n",
    "    global interested\n",
    "    global not_interested\n",
    "    global centroids\n",
    "    global tracked_person\n",
    "\n",
    "    if len(points) is 0:\n",
    "        for idx, centroid in enumerate(centroids):\n",
    "            centroid.gone_count += 1\n",
    "            if centroid.gone_count > MAX_FRAME_GONE:\n",
    "                remove_centroid(centroid.id)\n",
    "\n",
    "    if not centroids:\n",
    "        for idx, point in enumerate(points):\n",
    "            add_centroid(point)\n",
    "    else:\n",
    "        checked_points = len(points) * [None]\n",
    "        checked_points_dist = len(points) * [None]\n",
    "        for idx, point in enumerate(points):\n",
    "            p_id, dist = closest_centroid(point)\n",
    "            if dist > CENTROID_DISTANCE:\n",
    "                continue\n",
    "\n",
    "            if p_id in checked_points:\n",
    "                p_idx = checked_points.index(p_id)\n",
    "                if checked_points_dist[p_idx] > dist:\n",
    "                    checked_points[p_idx] = None\n",
    "                    checked_points_dist[p_idx] = None\n",
    "\n",
    "            checked_points[idx] = p_id\n",
    "            checked_points_dist[idx] = dist\n",
    "\n",
    "        for centroid in centroids:\n",
    "            if centroid.id in checked_points:\n",
    "                p_idx = checked_points.index(centroid.id)\n",
    "                centroid.point = points[p_idx]\n",
    "                centroid.gone_count = 0\n",
    "            else:\n",
    "                centroid.gone_count += 1\n",
    "                if centroid.gone_count > MAX_FRAME_GONE:\n",
    "                    remove_centroid(centroid.id)\n",
    "\n",
    "        for idx in range(len(checked_points)):\n",
    "            if checked_points[idx] is None:\n",
    "                add_centroid(points[idx])\n",
    "            else:\n",
    "                if looking[idx] is True:\n",
    "                    tracked_person[checked_points[idx]].sentiment = sentiment[idx]\n",
    "                    tracked_person[checked_points[idx]].looking += 1\n",
    "                    if sentiment[idx] == \"happy\" or sentiment[idx] == \"surprise\":\n",
    "                        tracked_person[checked_points[idx]].positive += 1\n",
    "                    elif sentiment[idx] == 'sad' or sentiment[idx] == 'anger':\n",
    "                        tracked_person[checked_points[idx]].negative += 1\n",
    "                    elif sentiment[idx] == 'neutral':\n",
    "                        tracked_person[checked_points[idx]].neutral += 1\n",
    "                else:\n",
    "                    tracked_person[checked_points[idx]].sentiment = \"Not looking\"\n",
    "\n",
    "        for person in tracked_person:\n",
    "            if person.counted is False:\n",
    "                positive = person.positive + person.neutral\n",
    "\n",
    "                # If the person is looking at the camera for specified time\n",
    "                # and his mood is positive, increment the interested variable\n",
    "                if (person.looking > fps * INTEREST_COUNT_TIME) and (positive > person.negative):\n",
    "                    interested += 1\n",
    "                    person.counted = True\n",
    "\n",
    "                # If the person is gone out of the frame, increment the not_interested variable\n",
    "                if person.gone is True:\n",
    "                    not_interested += 1\n",
    "                    person.counted = True\n",
    "\n",
    "\n",
    "def detect_head_pose_and_emotions(video_cap, object_det, cur_request_id_sh, next_request_id_sh):\n",
    "    \"\"\"\n",
    "    Detect the head pose and emotions of the faces detected\n",
    "\n",
    "    :param video_cap: VideoCap object\n",
    "    :param object_det: List of faces detected in the frame\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    global SENTIMENT_LABEL\n",
    "    global centroids\n",
    "    global is_async_mode\n",
    "\n",
    "    frame_centroids = []\n",
    "    looking = []\n",
    "    sentiment = []\n",
    "\n",
    "    for face in object_det:\n",
    "        xmin, ymin, xmax, ymax = face\n",
    "\n",
    "        # Find the centroid of the face\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        x = xmin + int(width / 2)\n",
    "        y = ymin + int(height / 2)\n",
    "        point = Point(x, y)\n",
    "        frame_centroids.append(point)\n",
    "\n",
    "        # Check the head pose\n",
    "        if is_async_mode:\n",
    "            head_pose = video_cap.next_frame[ymin:ymax, xmin:xmax]\n",
    "            in_frame = cv2.resize(head_pose, (video_cap.nchw_hp[3], video_cap.nchw_hp[2]))\n",
    "            in_frame = in_frame.transpose((2, 0, 1))\n",
    "            in_frame = in_frame.reshape((video_cap.nchw_hp[0], video_cap.nchw_hp[1],\n",
    "                                     video_cap.nchw_hp[2], video_cap.nchw_hp[3]))\n",
    "\n",
    "            video_cap.infer_network_hp.exec_net(video_cap.next_req, in_frame)\n",
    "        else:\n",
    "            head_pose = video_cap.frame[ymin:ymax, xmin:xmax]\n",
    "            in_frame = cv2.resize(head_pose, (video_cap.nchw_hp[3], video_cap.nchw_hp[2]))\n",
    "            in_frame = in_frame.transpose((2, 0, 1))\n",
    "            in_frame = in_frame.reshape((video_cap.nchw_hp[0], video_cap.nchw_hp[1],\n",
    "                                     video_cap.nchw_hp[2], video_cap.nchw_hp[3]))\n",
    "\n",
    "            video_cap.infer_network_hp.exec_net(video_cap.curr_req, in_frame)\n",
    " \n",
    "        if video_cap.infer_network_hp.wait(video_cap.curr_req) == 0:\n",
    "\n",
    "            # Parse head pose detection results\n",
    "            angle_p_fc = video_cap.infer_network_hp.get_output(video_cap.curr_req, \"angle_p_fc\")\n",
    "            angle_y_fc = video_cap.infer_network_hp.get_output(video_cap.curr_req, \"angle_y_fc\")\n",
    "\n",
    "            # Check if the person is looking at the camera\n",
    "            if (angle_y_fc > -22.5) & (angle_y_fc < 22.5) & (angle_p_fc > -22.5) & (angle_p_fc < 22.5):\n",
    "                looking.append(True)\n",
    "\n",
    "                # Find the emotions of the person\n",
    "                in_frame = cv2.resize(head_pose, (video_cap.nchw_md[3], video_cap.nchw_md[2]))\n",
    "                in_frame = in_frame.transpose((2, 0, 1))\n",
    "                in_frame = in_frame.reshape((video_cap.nchw_md[0], video_cap.nchw_md[1],\n",
    "                                             video_cap.nchw_md[2], video_cap.nchw_md[3]))\n",
    "                video_cap.infer_network_md.exec_net(0, in_frame)\n",
    "                video_cap.infer_network_md.wait(0)\n",
    "                res = video_cap.infer_network_md.get_output(0)\n",
    "                emotions = np.argmax(res)\n",
    "                sentiment.append(SENTIMENT_LABEL[emotions])\n",
    "            else:\n",
    "                looking.append(False)\n",
    "                sentiment.append(-1)\n",
    "\n",
    "        if is_async_mode:\n",
    "            video_cap.curr_req, video_cap.next_req = video_cap.next_req, video_cap.curr_req\n",
    "            video_cap.frame = video_cap.next_frame\n",
    "    update_centroid(frame_centroids, looking, sentiment, video_cap.vc.get(cv2.CAP_PROP_FPS))\n",
    "    for idx, centroid in enumerate(centroids):\n",
    "        cv2.rectangle(video_cap.frame, (centroid.point.x, centroid.point.y),\n",
    "                      (centroid.point.x + 1, centroid.point.y + 1), (0, 255, 0), 4, 16)\n",
    "        cv2.putText(video_cap.frame, \"person:{}\".format(centroid.id), (centroid.point.x + 1, centroid.point.y - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "\n",
    "def heatmap_generation(video_cap):\n",
    "    \"\"\"\n",
    "    Generates the heatmap\n",
    "\n",
    "    :param video_cap: VideoCap of input feed for which the heatmap has to be generated\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(video_cap.frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Remove the background\n",
    "    fgbgmask = video_cap.mog.apply(gray)\n",
    "\n",
    "    # Threshold the image\n",
    "    thresh = 2\n",
    "    max_value = 2\n",
    "    threshold_frame = cv2.threshold(fgbgmask, thresh, max_value, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # Add threshold image to the accumulated image\n",
    "    video_cap.accumulated_frame = cv2.add(threshold_frame, video_cap.accumulated_frame)\n",
    "    colormap_frame = cv2.applyColorMap(video_cap.accumulated_frame, cv2.COLORMAP_HOT)\n",
    "    video_cap.frame = cv2.addWeighted(video_cap.frame, 0.6, colormap_frame, 0.4, 0)\n",
    "\n",
    "\n",
    "def update_info_shopper(video_cap):\n",
    "    \"\"\"\n",
    "    Send \"shopper\" data to InfluxDB\n",
    "\n",
    "    :param video_cap: List of VideoCap object\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    global tracked_person\n",
    "    global interested\n",
    "    global not_interested\n",
    "    global db_client\n",
    "\n",
    "    json_body = [{\n",
    "        \"measurement\": \"{}_interest\".format(video_cap.type),\n",
    "        \"fields\": {\n",
    "            \"time\": time.time(),\n",
    "            \"Interested\": interested,\n",
    "            \"Not Interested\": not_interested,\n",
    "            \"Total Count\": len(tracked_person)\n",
    "        }\n",
    "    }]\n",
    "    db_client.write_points(json_body)\n",
    "    for person in tracked_person:\n",
    "        if person.gone is False:\n",
    "            tm = time.time() - person.in_time\n",
    "            looking_time = person.looking / video_cap.vc.get(cv2.CAP_PROP_FPS)\n",
    "            json_body = [{\n",
    "                \"measurement\": \"{}_duration\".format(video_cap.type),\n",
    "                \"fields\": {\n",
    "                    \"person\": person.id,\n",
    "                    \"Looking time\": looking_time,\n",
    "                    \"Time in frame\": tm,\n",
    "                    \"Current Mood\": person.sentiment\n",
    "                }\n",
    "            }]\n",
    "        db_client.write_points(json_body)\n",
    "\n",
    "\n",
    "def update_info_object(labels, video_cap):\n",
    "    \"\"\"\n",
    "    Send \"traffic\" and \"shelf\" data to InfluxDB\n",
    "\n",
    "    :param labels: List of labels present in label file\n",
    "    :param video_cap: VideoCap object\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    global db_client\n",
    "\n",
    "    for idx, label in enumerate(video_cap.labels_map):\n",
    "        json_body = [\n",
    "            {\"measurement\": video_cap.type,\n",
    "             \"tags\": {\n",
    "                 \"object\": labels[label],\n",
    "             },\n",
    "             \"fields\": {\n",
    "                 \"time\": time.time(),\n",
    "                 \"Current Count\": video_cap.current_count[idx],\n",
    "                 \"Total Count\": video_cap.total_count[idx],\n",
    "             }\n",
    "             }]\n",
    "        db_client.write_points(json_body)\n",
    "\n",
    "\n",
    "def create_database():\n",
    "    \"\"\"\n",
    "    Connect to InfluxDB and create the database\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    global db_client\n",
    "    global IPADDRESS\n",
    "    IPADDRESS = os.environ['DB_IPADDRESS'] if 'DB_IPADDRESS' in os.environ.keys() else \"localhost\"\n",
    "    proxy = {\"http\": \"http://{}:{}\".format(IPADDRESS, PORT)}\n",
    "    db_client = InfluxDBClient(host=IPADDRESS, port=PORT, proxies=proxy, database=DATABASE_NAME)\n",
    "    db_client.create_database(DATABASE_NAME)\n",
    "\n",
    "\n",
    "def retail_analytics():\n",
    "    \"\"\"\n",
    "    Detect objects on multiple input video feeds and process the output\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    global centroids\n",
    "    global tracked_person\n",
    "    global db_client\n",
    "    global is_async_mode, cur_request_id, next_request_id\n",
    "\n",
    "    objdetect = []\n",
    "    log.basicConfig(format=\"[ %(levelname)s ] %(message)s\", level=log.INFO, stream=sys.stdout)\n",
    "    logger = log.getLogger()\n",
    "\n",
    "    video_caps = parse_conf_file()\n",
    "    assert len(video_caps) != 0, \"No input source given in Configuration file\"\n",
    "    flag = os.environ['FLAG'] if 'FLAG' in os.environ.keys() else \"async\"\n",
    "    load_models(video_caps)\n",
    "    labels = get_used_labels(video_caps)\n",
    "    create_database()\n",
    "\n",
    "    min_fps = min([i.vc.get(cv2.CAP_PROP_FPS) for i in video_caps])\n",
    "    no_more_data = [False] * len(video_caps)\n",
    "    frames = [None] * len(video_caps)\n",
    "    start_time = time.time()\n",
    "    if flag == \"async\":\n",
    "        is_async_mode = True\n",
    "        print('Application running in async mode')\n",
    "    else:\n",
    "        is_async_mode = False\n",
    "        print('Application running in sync mode')\n",
    "    cur_request_id_tr = 0\n",
    "    next_request_id_tr = 1\n",
    "    cur_request_id = 0\n",
    "    next_request_id = 1\n",
    "    cur_request_id_sf = 0\n",
    "    next_request_id_sf = 1\n",
    "    cur_request_id_sh = 0\n",
    "    next_request_id_sh = 1\n",
    "    input_blob = [\"data\", \"im_info\"]\n",
    "    det_time = 0\n",
    "    cur_request_id = 0\n",
    "\n",
    "    # Main loop for object detection in multiple video streams\n",
    "    while True:\n",
    "        for idx, video_cap in enumerate(video_caps):\n",
    "            vfps = int(round(video_cap.vc.get(cv2.CAP_PROP_FPS)))\n",
    "            for i in range(0, int(round(vfps / min_fps))):\n",
    "                if is_async_mode:\n",
    "                    ret, video_cap.next_frame = video_cap.vc.read()\n",
    "                else:\n",
    "                    ret, video_cap.frame = video_cap.vc.read()\n",
    "\n",
    "                # If no new frame or error in reading the frame, exit the loop\n",
    "                if not ret:\n",
    "                    no_more_data[idx] = True\n",
    "                    break\n",
    "\n",
    "                if video_cap.type == 'traffic' or video_cap.type == 'shelf':\n",
    "                    video_cap.current_count = [0] * len(video_cap.labels)\n",
    "                    video_cap.changed_count = [False] * len(video_cap.labels)\n",
    "                inf_start = time.time()\n",
    "                if is_async_mode:\n",
    "                    in_frame = cv2.resize(video_cap.next_frame, (video_cap.nchw[3], video_cap.nchw[2]))\n",
    "                    in_frame = in_frame.transpose((2, 0, 1))\n",
    "                    in_frame = in_frame.reshape((video_cap.nchw[0], video_cap.nchw[1], video_cap.nchw[2], video_cap.nchw[3]))\n",
    "                    if video_cap.type == 'traffic':\n",
    "                        video_cap.infer_network.exec_net(next_request_id_tr, in_frame, input_blob, video_cap.vc.get(3), video_cap.vc.get(4))\n",
    "                        cur_request_id = cur_request_id_tr\n",
    "                    elif video_cap.type == 'shelf':\n",
    "                        video_cap.infer_network.exec_net(next_request_id_sf, in_frame)\n",
    "                        cur_request_id = cur_request_id_sf\n",
    "                    else:\n",
    "                        video_cap.infer_network.exec_net(next_request_id_sh, in_frame)\n",
    "                        cur_request_id = cur_request_id_sh\n",
    "                    video_cap.frame = video_cap.next_frame\n",
    "                else:\n",
    "                    in_frame = cv2.resize(video_cap.frame, (video_cap.nchw[3], video_cap.nchw[2]))\n",
    "                    in_frame = in_frame.transpose((2, 0, 1))\n",
    "                    in_frame = in_frame.reshape((video_cap.nchw[0], video_cap.nchw[1], video_cap.nchw[2], video_cap.nchw[3]))\n",
    "                    if video_cap.type == 'traffic':\n",
    "                        video_cap.infer_network.exec_net(cur_request_id_tr, in_frame, input_blob, video_cap.vc.get(3), video_cap.vc.get(4))\n",
    "                    elif video_cap.type == 'shelf':\n",
    "                        video_cap.infer_network.exec_net(cur_request_id_sf, in_frame)\n",
    "                    else:\n",
    "                        video_cap.infer_network.exec_net(cur_request_id_sh, in_frame)\n",
    "                if video_cap.infer_network.wait(cur_request_id) == 0:\n",
    "                    inf_end = time.time()\n",
    "                    det_time = inf_end - inf_start\n",
    "\n",
    "                    # Pass the frame to the inference engine and get the results\n",
    "                    res = video_cap.infer_network.get_output(cur_request_id)\n",
    "\n",
    "                    # Process the result obtained from the inference engine\n",
    "                    object_det = object_detection(video_cap, res)\n",
    "\n",
    "                    # If the feed type is \"traffic\" or \"shelf\", check the current and total count of the object\n",
    "                    if video_cap.type == 'traffic' or video_cap.type == 'shelf':\n",
    "                        process_output(video_cap)\n",
    "\n",
    "                        # If feed type is \"traffic\", generate the heatmap\n",
    "                        if video_cap.type == 'traffic':\n",
    "                            heatmap_generation(video_cap)\n",
    "\n",
    "                        # Send the data to InfluxDB\n",
    "                        if time.time() >= video_cap.utime + 1:\n",
    "                            update_info_object(labels, video_cap)\n",
    "                            video_cap.utime = time.time()\n",
    "\n",
    "                    else:\n",
    "                        # Detect head pose and emotions of the faces detected\n",
    "                        detect_head_pose_and_emotions(video_cap, object_det, cur_request_id_sh, next_request_id_sh)\n",
    "\n",
    "                        # Send the data to InfluxDB\n",
    "                        if time.time() >= video_cap.utime + 1:\n",
    "                            update_info_shopper(video_cap)\n",
    "                            video_cap.utime = time.time()\n",
    "\n",
    "                if is_async_mode:\n",
    "                    if video_cap.type == 'traffic':\n",
    "                        cur_request_id_tr, next_request_id_tr = next_request_id_tr, cur_request_id_tr\n",
    "                    elif video_cap.type == 'shopper':\n",
    "                        cur_request_id_sh, next_request_id_sh = next_request_id_sh, cur_request_id_sh\n",
    "                    else:\n",
    "                        cur_request_id_sf, next_request_id_sf = next_request_id_sf, cur_request_id_sf                \n",
    "\n",
    "                fps_time = time.time() - start_time\n",
    "                fps_message = \"FPS: {:.3f} fps\".format(1 / fps_time)\n",
    "                start_time = time.time()\n",
    "                inf_time_message = \"Inference time: N\\A for async mode\" if is_async_mode else\\\n",
    "                \"Inference time: {:.3f} ms\".format(det_time * 1000)\n",
    "                cv2.putText(video_cap.frame, inf_time_message, (10, 15), cv2.FONT_HERSHEY_COMPLEX, 0.5,\n",
    "                            (200, 10, 10), 1)\n",
    "                cv2.putText(video_cap.frame, fps_message, (10, int(video_cap.input_height) - 10),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (200, 10, 10), 1)\n",
    "\n",
    "            # If no new frame, continue to the next input feed\n",
    "            if no_more_data[idx] is True:\n",
    "                continue\n",
    "\n",
    "            # Print the results on the frame and stream it\n",
    "            message = \"Feed Type: {}\".format(video_cap.type)\n",
    "            cv2.putText(video_cap.frame, message, (10, 30),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX, 0.5, (200, 10, 10), 1)\n",
    "\n",
    "            if video_cap.type == 'traffic' or video_cap.type == 'shelf':\n",
    "                ht = 50\n",
    "                for indx, label in enumerate(video_cap.labels_map):\n",
    "                    message = \"{} -> Total Count: {}, Current Count: {}\".format(labels[label],\n",
    "                                                                                video_cap.total_count[indx],\n",
    "                                                                                video_cap.current_count[indx])\n",
    "                    cv2.putText(video_cap.frame, message, (10, ht), cv2.FONT_HERSHEY_COMPLEX, 0.5, (200, 10, 10), 1)\n",
    "                    ht += 20\n",
    "            else:\n",
    "                message = \"Face -> Total Count: {}, Current Count: {}\".format(len(tracked_person), len(centroids))\n",
    "                cv2.putText(video_cap.frame, message, (10, 50), cv2.FONT_HERSHEY_COMPLEX, 0.5, (200, 10, 10), 1)\n",
    "                ht = 75\n",
    "                for person in tracked_person:\n",
    "                    if person.gone is False:\n",
    "                        message = \"Person {} is {}\".format(person.id, person.sentiment)\n",
    "                        cv2.putText(video_cap.frame, message, (10, ht), cv2.FONT_HERSHEY_COMPLEX, 0.5, (200, 10, 10), 1)\n",
    "                        ht += 20\n",
    "\n",
    "            frames[idx] = video_cap.frame\n",
    "\n",
    "        # Resize the processed frames to stream on Grafana\n",
    "        for idx, img in enumerate(frames):\n",
    "            frames[idx] = cv2.resize(img, (480, 360))\n",
    "\n",
    "        # Encode the frames into a memory buffer.\n",
    "        ret, img = cv2.imencode('.jpg', np.hstack(frames))\n",
    "        img = img.tobytes()\n",
    "\n",
    "        # Yield the output frame to the server\n",
    "        yield (b'--frame\\r\\n'\n",
    "               b'Content-Type: image/jpeg\\r\\n\\r\\n' + img + b'\\r\\n\\r\\n')\n",
    "\n",
    "        # If no more frames, exit the loop\n",
    "        if False not in no_more_data:\n",
    "            break\n",
    "\n",
    "\n",
    "# Create object for Flask class\n",
    "app = Flask(__name__, template_folder=template_dir)\n",
    "\n",
    "\n",
    "# Trigger the index() function on opening \"0.0.0.0:5000/\" URL\n",
    "@app.route('/')\n",
    "def index():\n",
    "    \"\"\"\n",
    "    Trigger the index() function on opening \"0.0.0.0:5000/\" URL\n",
    "    :return: html file\n",
    "    \"\"\"\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "# Trigger the video_feed() function on opening \"0.0.0.0:5000/video_feed\" URL\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    \"\"\"\n",
    "    Trigger the video_feed() function on opening \"0.0.0.0:5000/video_feed\" URL\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return Response(retail_analytics(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
